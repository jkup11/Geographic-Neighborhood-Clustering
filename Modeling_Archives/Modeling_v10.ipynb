{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Josh Kuppersmith\n",
    "\n",
    "Date: March 29, 2019\n",
    "\n",
    "Subject: Modeling Notebook, Homogeneity Metrics, Clustering, and Optimizing\n",
    "\n",
    "*Working version from new repository, adding feedback from Prof Rycroft*\n",
    "\n",
    "Encompasses changes with PCA and Centroid Initialization, As well as visuals that explain why there are probles and how to fix them. \n",
    "\n",
    "Advisor: Pavlos Protopapas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Functions\n",
    "from Functions import sd, emd, hav_dist, pure_geo_k_means, geo_k_means, \n",
    "from Functions import geo_k_means_pca_3, geo_k_means_pca_2, geo_k_means_pca_1\n",
    "from Functions import regularize, silhouette, optimal_k_run\n",
    "from Functions import evaluate_model\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction/Notebook Description: \n",
    "\n",
    "In this notebook, we complete both baseline and project models. These are clustering models. A key piece of this is the metric by which we measure success of clustering. For this we use earth moving distance to measure tightness of cluster, which is focused on crime profiles across cells in a given neighborhood.\n",
    "\n",
    "#### Running Instructions: \n",
    "\n",
    "Run like normal. First section will load the data from our pre-processing and EDA, then the rest runs models, with bolded headers between each so that we can run them separately if we want. See previous notebooks (or writeup) for more detailed information about models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Accumulated final gridded datasets\n",
    "crime_grid = pd.read_csv(\"Cleaned/Accumulated_Crime_15_18.csv\")\n",
    "feature_grid = pd.read_csv(\"Cleaned/Accumulated_Grid_Features.csv\")\n",
    "full_grid = pd.read_csv(\"Cleaned/Merged_Grid_Data.csv\")\n",
    "clustering_grid = pd.read_csv('Cleaned/Accumulated_Grid_10Yr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Grid: \" + str(len(clustering_grid)) + \" cells\")\n",
    "clustering_grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Standard Deviation Measure for Within-Cluster Inhomogeneity (Replacing Entropy): \n",
    "\n",
    "Higher entropy means that there is more inhomogeneity in the neighborhood\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in data and the column where it finds cluster labels\n",
    "# and returns the sizes of each \n",
    "def sd(data, cluster_col):\n",
    "    e_data = data[data[cluster_col] > -1]\n",
    "    max_clust = np.max(list(e_data[cluster_col]))\n",
    "    sd_list = []\n",
    "                       \n",
    "    for i in list(set(list(e_data[cluster_col]))):\n",
    "        clust_data = e_data[e_data[cluster_col] == i]\n",
    "        sd = math.sqrt(np.var(list(clust_data['All Crime'])))\n",
    "        sd_list.append(sd)\n",
    "        \n",
    "    return sd_list\n",
    "\n",
    "sd_list = sd(clustering_grid, 'Neighborhoods')\n",
    "for i in range(10):\n",
    "    print(sd_list[i], len(clustering_grid[clustering_grid.Neighborhoods == i]))\n",
    "print\n",
    "print(\"We can see that small neighborhoods tend to have very small standard deviation which makes sense\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.hist(sd_list, bins=30)\n",
    "plt.title(\"Histogram of Standard Deviation of Neighborhoods\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Standard Deviation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood 5, 12, and 17 analysis\n",
    "\n",
    "print(\"Here are 3 neighborhoods with very different Standard Deviation values for 'All Crime.'\")\n",
    "print(\"These are the same as the ones we will use to calculate EMD. These illustrate the way that\")\n",
    "print(\"SD handles scale of data, the spread of the distribution, and multimodality of distributions.\")\n",
    "print\n",
    "#print(len(clustering_grid[clustering_grid.Neighborhoods == 5]))\n",
    "print(\"Neighborhood 5 (size 51) All Crime SD: \" + str(sd_list[5]))\n",
    "print(\"Neighborhood 12 (size 34) All Crime SD: \" + str(sd_list[12]))\n",
    "print(\"Neighborhood 17 (size 58) All Crime SD: \" + str(sd_list[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(16,8))\n",
    "axs[0].hist(clustering_grid[clustering_grid.Neighborhoods == 5]['All Crime'], bins=20)\n",
    "axs[0].set_title('Neighborhood 5: Size 51')\n",
    "axs[1].hist(clustering_grid[clustering_grid.Neighborhoods == 12]['All Crime'], bins=20)\n",
    "axs[1].set_title('Neighborhood 12: Size 34')\n",
    "axs[2].hist(clustering_grid[clustering_grid.Neighborhoods == 17]['All Crime'], bins=20)\n",
    "axs[2].set_title('Neighborhood 17: Size 58')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be interesting to see how this compares to EMD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Earth Mover's Distance for Measuring Within-Cluster Similarity\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emd(data, cluster_col):\n",
    "    clusters = data[cluster_col]\n",
    "    max_clust = np.max(list(clusters))\n",
    "    \n",
    "    cluster_emd_vecs = []\n",
    "    # iterate through clusters\n",
    "    for cluster in range(max_clust+1):\n",
    "        cluster_data = data[data[cluster_col] == cluster]\n",
    "        cluster_size = len(cluster_data)\n",
    "        if cluster_size == 0:\n",
    "            #print(\"Cluster Size 0 error\")\n",
    "            continue\n",
    "        \n",
    "        crime_list = list(cluster_data['All Crime'])\n",
    "        neighborhood_mean = np.mean(crime_list)\n",
    "        sq_mu = math.sqrt(neighborhood_mean)\n",
    "        \n",
    "        sample_emd = []\n",
    "        # average to eliminate impact of randomness on calculation\n",
    "        # make result more consistent\n",
    "        for j in range(50):\n",
    "            comparison_dist = []\n",
    "            for i in range(len(crime_list)):\n",
    "                comparison_dist.append(neighborhood_mean + np.random.uniform(-1.0*sq_mu, sq_mu))\n",
    "            sample_emd.append(emd_samples(crime_list, comparison_dist))\n",
    "        cluster_emd_vecs.append(np.mean(sample_emd))\n",
    "\n",
    "    return cluster_emd_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = emd(clustering_grid[clustering_grid.Neighborhoods > -1], 'Neighborhoods')\n",
    "averages2 = emd(clustering_grid[clustering_grid.Beats > -1], 'Beats')\n",
    "\n",
    "print(\"Average EMD for several crime types for 'Neighborhoods': \" + str(np.mean(averages)))\n",
    "print(\"Average EMD for several crime types for 'Beats': \" + str(np.mean(averages2)))\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "axs[0].hist(averages, bins=30)\n",
    "axs[1].hist(averages2, bins=30)\n",
    "axs[0].set_title(\"Neighborhood\")\n",
    "axs[1].set_title(\"Beats\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood 5, 12, and 17 analysis\n",
    "\n",
    "print(\"Here are 3 neighborhoods with very different EMD values for 'All Crime'. These illustrate the way that\")\n",
    "print(\"EMD scale of data, the spread of the distribution, and multimodality of distributions.\")\n",
    "print\n",
    "#print(len(clustering_grid[clustering_grid.Neighborhoods == 5]))\n",
    "print(\"Neighborhood 5 (size 51) All Crime EMD: \" + str(averages[5]))\n",
    "print(\"Neighborhood 12 (size 34) All Crime EMD: \" + str(averages[12]))\n",
    "print(\"Neighborhood 12 (size 58) All Crime EMD: \" + str(averages[17]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(16,8))\n",
    "axs[0].hist(clustering_grid[clustering_grid.Neighborhoods == 5]['All Crime'], bins=20)\n",
    "axs[0].set_title('Neighborhood 5: EMD 1082')\n",
    "axs[1].hist(clustering_grid[clustering_grid.Neighborhoods == 12]['All Crime'], bins=20)\n",
    "axs[1].set_title('Neighborhood 12: EMD 418')\n",
    "axs[2].hist(clustering_grid[clustering_grid.Neighborhoods == 17]['All Crime'], bins=20)\n",
    "axs[2].set_title('Neighborhood 17: EMD 204')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMD Punishes multimodality more so that is why we will use it. Multi-modality is really bad for trying to create homogeneous neighborhoods that we can use to differetiate areas. Also, looking at some work, it seems that the difference between SD mean and median is usually greater than for EMD, and given that either of these metrics could work, I trust EMD more because it is more consistent between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Testing with PCA\n",
    "\n",
    "PCA, Principle Component Analysis, uses linear algebra transformations on data to convert correlated features into a set of uncorellated variables - principal components. These help to extract more meaning from correlated features, and they take into account initial scale of variables, so it would be interesting to see what happens to the crime features. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = list(clustering_grid.columns.values)\n",
    "pca_columns = all_columns\n",
    "for val in [\"Unnamed: 0\", \"Latitude\", \"Longitude\", \"lat\", \"lon\", \"coord\", \"Neighborhoods\", \"Beats\"]:\n",
    "    pca_columns.remove(val)\n",
    "print(pca_columns)\n",
    "print\n",
    "print(\"Total # Feature Columns: \" + str(len(pca_columns)))\n",
    "print(\"Dimensionality reduction would be useful here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(clustering_grid[pca_columns])\n",
    "\n",
    "NUM = 5\n",
    "pca = PCA(n_components=NUM)\n",
    "pca.fit(X)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(explained_var)\n",
    "\n",
    "total_var = []\n",
    "for i in range(NUM):\n",
    "    total_var.append(np.sum(explained_var[0:i+1]))\n",
    "print(total_var)\n",
    "\n",
    "#f, axes = plt.subplots(1, 1, figsize=(10, 10), sharex=False)\n",
    "plt.plot(range(1,NUM+1), total_var)\n",
    "plt.title(\"PCA Explained Variance\")\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel(\"Total Explained Variance\")\n",
    "plt.ylim([0.9,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "\n",
    "clustering_grid['PCA_1'] = X_pca[:,0]\n",
    "clustering_grid['PCA_2'] = X_pca[:,1]\n",
    "clustering_grid['PCA_3'] = X_pca[:,2]\n",
    "clustering_grid['PCA_4'] = X_pca[:,3]\n",
    "clustering_grid['PCA_5'] = X_pca[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Maps of PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps\n",
    "import gmaps.datasets\n",
    "gmaps.configure(api_key='AIzaSyDLK6eRC5Wd_ibqBhNsFAnYSdSH5MbuZ8U')\n",
    "\n",
    "lats = [x for x in clustering_grid['Latitude']]\n",
    "lons = [x for x in clustering_grid['Longitude']]\n",
    "weights = [x for x in clustering_grid['PCA_1']]\n",
    "#normalize weights to [0,1]\n",
    "normalized_weights = (np.array(weights)-min(weights))/(max(weights)-min(weights))\n",
    "\n",
    "my_locations = []\n",
    "for i in range(len(lats)):\n",
    "    my_locations.append((lats[i], lons[i], normalized_weights[i]))\n",
    "    \n",
    "fig = gmaps.Map()\n",
    "fig.add_layer(gmaps.WeightedHeatmap(data=my_locations))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps\n",
    "import gmaps.datasets\n",
    "gmaps.configure(api_key='AIzaSyDLK6eRC5Wd_ibqBhNsFAnYSdSH5MbuZ8U')\n",
    "\n",
    "lats = [x for x in clustering_grid['Latitude']]\n",
    "lons = [x for x in clustering_grid['Longitude']]\n",
    "weights = [x for x in clustering_grid['PCA_2']]\n",
    "#normalize weights to [0,1]\n",
    "normalized_weights = (np.array(weights)-min(weights))/(max(weights)-min(weights))\n",
    "\n",
    "my_locations = []\n",
    "for i in range(len(lats)):\n",
    "    my_locations.append((lats[i], lons[i], normalized_weights[i]))\n",
    "    \n",
    "fig = gmaps.Map()\n",
    "fig.add_layer(gmaps.WeightedHeatmap(data=my_locations, max_intensity=2, point_radius=10))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Baseline 1 - City Neighborhood Boundaries\n",
    "\n",
    "### Get baseline numbers, visuals of all crime EMD and Entropy\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue='Neighborhoods', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhoods are pretty irregular, but mostly not jagged or disconnected. Important things to note for the regularization step, which is meant to enforce contiguousness in the neighborhood boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Neighborhoods: \" + str(len(np.unique(list(clustering_grid.Neighborhoods)))))\n",
    "\n",
    "start1 = time.time()\n",
    "all_c_emd = emd(clustering_grid[clustering_grid.Neighborhoods > -1], 'Neighborhoods')\n",
    "emd_time = time.time() - start1\n",
    "\n",
    "start2  = time.time()\n",
    "all_c_sd = sd(clustering_grid[clustering_grid.Neighborhoods > -1], 'Neighborhoods')\n",
    "sd_time = time.time() - start2\n",
    "\n",
    "print\n",
    "print(\"The average neighborhood all crime EMD is: \" + str(np.mean(all_c_emd)) + \", the median is: \" + str(np.median(all_c_emd)) + \", the maximum is: \" + str(np.max(all_c_emd)) + \", and the minimum is: \" + str(np.min(all_c_emd)))\n",
    "print(\"EMD Calculation took: \" + str(emd_time))\n",
    "print\n",
    "print(\"The average neighborhood all crime Standard Deviation is: \" + str(np.mean(all_c_sd)) + \", the median is: \" + str(np.median(all_c_sd)) + \", the maximum is: \" + str(np.max(all_c_sd)) + \", and the minimum is: \" + str(np.min(all_c_sd)))\n",
    "print(\"Variance Calculation took: \" + str(sd_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've learned a few things. First, median and mean are very different for these across neighborhood calculations. Median seems to be the better way to go, since huge numbers are skewing this in both cases, though we should figure out where the huge and tiny numbers are coming from (sizes of neighborhoods? location). Also, we learned that an Entropy calculation takes ~8x longer to run than entropy, so maybe for the sake of training, EMD is the better way to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,7))\n",
    "ax[0].hist(all_c_emd, bins=30)\n",
    "ax[0].set_title(\"Histogram of Neighborhood EMD\")\n",
    "ax[1].hist(all_c_sd, bins=30)\n",
    "ax[1].set_title(\"Histogram of Neighborhood SD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both are influenced by outliers, the they look very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Baseline 2 - Police Beats\n",
    "\n",
    "### Get baseline numbers, visuals of all crime EMD\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue='Beats', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Beats: \" + str(len(np.unique(list(clustering_grid.Beats)))))\n",
    "\n",
    "start1 = time.time()\n",
    "all_c_emd_beat = emd(clustering_grid[clustering_grid.Beats > -1], 'Beats')\n",
    "emd_time = time.time() - start1\n",
    "\n",
    "start2  = time.time()\n",
    "all_c_sd_beat = sd(clustering_grid[clustering_grid.Beats > -1], 'Beats')\n",
    "var_time = time.time() - start2\n",
    "\n",
    "print\n",
    "print(\"The average Beat all crime EMD is: \" + str(np.mean(all_c_emd_beat)) + \", the median is: \" + str(np.median(all_c_emd_beat)) + \", the maximum is: \" + str(np.max(all_c_emd_beat)) + \", and the minimum is: \" + str(np.min(all_c_emd_beat)))\n",
    "print(\"EMD Calculation took: \" + str(emd_time))\n",
    "print\n",
    "print(\"The average Beat all crime Standard Deviation is: \" + str(np.mean(all_c_sd_beat)) + \", the median is: \" + str(np.median(all_c_sd_beat)) + \", the maximum is: \" + str(np.max(all_c_sd_beat)) + \", and the minimum is: \" + str(np.min(all_c_sd_beat)))\n",
    "print(\"Variance Calculation took: \" + str(sd_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,7))\n",
    "ax[0].hist(all_c_emd_beat, bins=30)\n",
    "ax[0].set_title(\"Histogram of Beat EMD\")\n",
    "ax[1].hist(all_c_sd_beat, bins=30)\n",
    "ax[1].set_title(\"Histogram of Beat SD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks surprisingly normal for beats! Could be a good feature. One note is that since there are so many beats, it will be logistically tough to run models with k=~270 that can be used to compare against this baseline, so Neighborhoods is probably a stronger baseline. Also, in terms of the narrative, Neighborhoods probably makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Naive K-Means Geographic Clustering\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive geographical clustering just using latitude and longitude with sklearn kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_simple = [\"lat\", \"lon\"]\n",
    "\n",
    "X = clustering_grid[predictors_simple]\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit(X)\n",
    "y_km = kmeans.fit_predict(X)\n",
    "clustering_grid['Cluster'] = y_km\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.lmplot(x=\"lon\", y=\"lat\", data=clustering_grid, fit_reg=False, hue='Cluster', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "all_c_emd_clust = emd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "all_c_sd_clust = sd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "print(\"The average Cluster all crime EMD is: \" + str(np.mean(all_c_emd_clust)) + \", the median is: \" + str(np.median(all_c_emd_clust)) + \", the maximum is: \" + str(np.max(all_c_emd_clust)) + \", and the minimum is: \" + str(np.min(all_c_emd_clust)))\n",
    "print\n",
    "print(\"The average Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd_clust)) + \", the median is: \" + str(np.median(all_c_sd_clust)) + \", the maximum is: \" + str(np.max(all_c_sd_clust)) + \", and the minimum is: \" + str(np.min(all_c_sd_clust)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another naive attempt using sklearn Kmeans on lat, lon, and cell towers - messes the whole thing up. Likely due to scale, would be interesting to see the results if we were to normalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"Cell Towers\", \"lat\", \"lon\"]\n",
    "\n",
    "X = clustering_grid[predictors]\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit(X)\n",
    "y_km = kmeans.fit_predict(X)\n",
    "clustering_grid['Cluster'] = y_km\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.lmplot(x=\"lon\", y=\"lat\", data=clustering_grid, fit_reg=False, hue='Cluster', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "all_c_emd_clust = emd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "all_c_sd_clust = sd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "print(\"The average Cluster all crime EMD is: \" + str(np.mean(all_c_emd_clust)) + \", the median is: \" + str(np.median(all_c_emd_clust)) + \", the maximum is: \" + str(np.max(all_c_emd_clust)) + \", and the minimum is: \" + str(np.min(all_c_emd_clust)))\n",
    "print\n",
    "print(\"The average Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd_clust)) + \", the median is: \" + str(np.median(all_c_sd_clust)) + \", the maximum is: \" + str(np.max(all_c_sd_clust)) + \", and the minimum is: \" + str(np.min(all_c_sd_clust)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize features then run the above naive k-means to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"Cell Towers\", \"lat\", \"lon\"]\n",
    "\n",
    "X = clustering_grid[predictors]\n",
    "normalized_X=(X-X.mean())/X.std()\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit(normalized_X)\n",
    "y_km = kmeans.fit_predict(normalized_X)\n",
    "clustering_grid['Cluster'] = y_km\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.lmplot(x=\"lon\", y=\"lat\", data=clustering_grid, fit_reg=False, hue='Cluster', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "all_c_emd_clust = emd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "all_c_sd_clust = sd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "print(\"The average Cluster all crime EMD is: \" + str(np.mean(all_c_emd_clust)) + \", the median is: \" + str(np.median(all_c_emd_clust)) + \", the maximum is: \" + str(np.max(all_c_emd_clust)) + \", and the minimum is: \" + str(np.min(all_c_emd_clust)))\n",
    "print\n",
    "print(\"The average Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd_clust)) + \", the median is: \" + str(np.median(all_c_sd_clust)) + \", the maximum is: \" + str(np.max(all_c_sd_clust)) + \", and the minimum is: \" + str(np.min(all_c_sd_clust)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These results actually don't look too bad, but if we add more features, the neighborhoods stop making sense, illustrating the need for our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"Cell Towers\", \"restaurant\", \"Bus Stop\", \"parking\", \"lat\", \"lon\"]\n",
    "\n",
    "X = clustering_grid[predictors]\n",
    "normalized_X=(X-X.mean())/X.std()\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit(normalized_X)\n",
    "y_km = kmeans.fit_predict(normalized_X)\n",
    "clustering_grid['Cluster'] = y_km\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.lmplot(x=\"lon\", y=\"lat\", data=clustering_grid, fit_reg=False, hue='Cluster', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "all_c_emd_clust = emd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "all_c_sd_clust = sd(clustering_grid[clustering_grid.Cluster > -1], 'Cluster')\n",
    "print(\"The average Cluster all crime EMD is: \" + str(np.mean(all_c_emd_clust)) + \", the median is: \" + str(np.median(all_c_emd_clust)) + \", the maximum is: \" + str(np.max(all_c_emd_clust)) + \", and the minimum is: \" + str(np.min(all_c_emd_clust)))\n",
    "print\n",
    "print(\"The average Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd_clust)) + \", the median is: \" + str(np.median(all_c_sd_clust)) + \", the maximum is: \" + str(np.max(all_c_sd_clust)) + \", and the minimum is: \" + str(np.min(all_c_sd_clust)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Pure Geographic Clustering Function\n",
    "\n",
    "Instructions for goal are above. Upon researching online, it seems that it may not be possible to use sklearn to define my own distance and centroid function, so I will probably have to implement this myself. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def hav_dist(lat1, lon1, lat2, lon2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    c = 2 * asin(sqrt(sin((lat2 - lat1) /2)**2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1) /2)**2)) * 6371 # Radius of earth in kilometers\n",
    "    return c\n",
    "\n",
    "#def hav_dist_2(lat1, lon1, lat2, lon2):\n",
    "#    s15 = time.time()\n",
    "#    coords_1 = (lat1, lon1)\n",
    "#    coords_2 = (lat2, lon2)\n",
    "#    d = geopy.distance.distance(coords_1, coords_2).km\n",
    "#    print(\"Hav2 took: \" + str(time.time()-s15))\n",
    "#    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hav1 is fastest, seems tough to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_data = clustering_grid[['Latitude', 'Longitude']]\n",
    "\n",
    "def pure_geo_k_means(data, k, cap=1000, verbose=False):\n",
    "    # store error for iteration analysis\n",
    "    s1 = time.time()\n",
    "    error_mags = []\n",
    "    sse = []\n",
    "    C_x = []\n",
    "    C_y = []\n",
    "    rand_inds = random.sample(range(0, len(data)), k)\n",
    "\n",
    "    arr = data.iloc[rand_inds][['Latitude','Longitude']]\n",
    "    centroids_curr = np.array(arr, dtype=np.float32)\n",
    "    # centroid storage\n",
    "    centroids_old = np.zeros((k,2))\n",
    "    # cluster label\n",
    "    clusters = np.zeros(len(data))\n",
    "    ideal_error = list(np.zeros(k))\n",
    "    error = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # lats and lons for haversine distance\n",
    "        error.append(hav_dist(centroids_old[i][0], centroids_old[i][1], centroids_curr[i][0], centroids_curr[i][1]))\n",
    "    itera = 0\n",
    "    while not error == ideal_error:\n",
    "        s2 = time.time()\n",
    "        itera += 1\n",
    "        \n",
    "        # stop early\n",
    "        if itera > cap:\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: \" + str(itera))\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            distances = []\n",
    "            lat = data.at[i, 'Latitude']\n",
    "            lon = data.at[i, 'Longitude']\n",
    "            for j in range(len(centroids_curr)):\n",
    "                distances.append(hav_dist(centroids_curr[j][0], centroids_curr[j][1], lat, lon))\n",
    "            cluster_num = np.argmin(distances)\n",
    "            clusters[i] = cluster_num\n",
    "        # Store old centroid values\n",
    "        C_old_store = deepcopy(centroids_curr)\n",
    "        data.temp_clust = clusters\n",
    "        \n",
    "        sse_iter = 0\n",
    "        for i in range(k):\n",
    "            points_in_clust = data[data.temp_clust == i][['Latitude', 'Longitude']]\n",
    "            point_mean = np.mean(points_in_clust, axis=0)\n",
    "            centroids_curr[i] = point_mean\n",
    "            lat = point_mean[0]\n",
    "            lon = point_mean[1]\n",
    "            matrix = np.array(points_in_clust)\n",
    "            lats = matrix[:,0]\n",
    "            lons = matrix[:,1]\n",
    "            \n",
    "            for j in range(len(matrix)):\n",
    "                sse_iter = sse_iter + hav_dist(lat, lon, lats[j], lons[j])\n",
    "        sse.append(sse_iter)\n",
    "            \n",
    "        error = []\n",
    "        for i in range(k):\n",
    "            error.append(hav_dist(C_old_store[i][0], C_old_store[i][1], centroids_curr[i][0], centroids_curr[i][1]))\n",
    "        er = np.linalg.norm(error)\n",
    "        error_mags.append(er)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Magnitude of error: \" + str(er))\n",
    "            print(\"Iteration took: \" + str(time.time()-s2))\n",
    "            print\n",
    "    data['CLUSTER_LABEL'] = clusters\n",
    "    print(\"Done. Total Time: \" + str(time.time() - s1))\n",
    "    \n",
    "    return data[['Latitude', 'Longitude', 'CLUSTER_LABEL']], error_mags, sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d, error_mags, sse = pure_geo_k_means(clustering_data, 10,verbose=False)\n",
    "new_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].plot(range(len(error_mags)), error_mags)\n",
    "axs[0].set_xlabel('Iteration Number')\n",
    "axs[0].set_ylabel('Magnitude of Iteration Error Vector')\n",
    "axs[0].set_title('Convergence Analysis (Using Movement of Centroids)')\n",
    "axs[1].plot(range(len(sse)), sse)\n",
    "axs[1].set_xlabel('Iteration Number')\n",
    "axs[1].set_ylabel('Magnitude of Iteration SSE')\n",
    "axs[1].set_title('Convergence Analysis (SSE, Should Decrease)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check EMD and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_grid['SIMPLE_CLUSTER_LABEL'] = [int(i) for i in list(new_d.CLUSTER_LABEL)]\n",
    "all_c_emd2 = emd(clustering_grid, 'SIMPLE_CLUSTER_LABEL')\n",
    "print(\"The average Simple Cluster all crime EMD is: \" + str(np.mean(all_c_emd2)) + \", the median is: \" + str(np.median(all_c_emd2))  + \", the maximum is: \" + str(np.max(all_c_emd2)) + \", and the minimum is: \" + str(np.min(all_c_emd2)))\n",
    "print\n",
    "print(all_c_emd2)\n",
    "print\n",
    "for i in range(10):\n",
    "    print(len(clustering_grid[clustering_grid.SIMPLE_CLUSTER_LABEL==i]))\n",
    "print\n",
    "print('--------------------------------------------------------------')\n",
    "print\n",
    "all_c_sd2 = sd(clustering_grid, 'SIMPLE_CLUSTER_LABEL')\n",
    "print(\"The average Simple Cluster all crime standard deviation is: \" + str(np.mean(all_c_sd2)) + \", the median is: \" + str(np.median(all_c_sd2))  + \", the maximum is: \" + str(np.max(all_c_sd2)) + \", and the minimum is: \" + str(np.min(all_c_sd2)))\n",
    "print\n",
    "print(all_c_sd2)\n",
    "print\n",
    "for i in range(10):\n",
    "    print(len(clustering_grid[clustering_grid.SIMPLE_CLUSTER_LABEL==i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consistently sized and relatively consistently large EMDs. Results make sense to me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try again quickly with much higher value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d2, error_mags2, sse2 = pure_geo_k_means(clustering_data, 97, verbose=False)\n",
    "# Running this showed me that in many cases, the initial error magnitude\n",
    "new_d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d2, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(range(len(error_mags2)), error_mags2)\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Magnitude of Iteration Error Vector')\n",
    "plt.title('Convergence Analysis (Using Movement of Centroids)')\n",
    "plt.show()\n",
    "\n",
    "clustering_grid['SIMPLE_CLUSTER_LABEL_97'] = [int(i) for i in list(new_d2.CLUSTER_LABEL)]\n",
    "all_c_emd3 = emd(clustering_grid, 'SIMPLE_CLUSTER_LABEL_97')\n",
    "print(\"The average Simple Cluster all crime EMD (97 clusters) is: \" + str(np.mean(all_c_emd3)) + \", the median is: \" + str(np.median(all_c_emd3))  + \", the maximum is: \" + str(np.max(all_c_emd3)) + \", and the minimum is: \" + str(np.min(all_c_emd3)))\n",
    "print\n",
    "print('-----------------------')\n",
    "print\n",
    "all_c_sd3 = sd(clustering_grid, 'SIMPLE_CLUSTER_LABEL_97')\n",
    "print(\"The average Simple Cluster all crime standard deviation is: \" + str(np.mean(all_c_sd3)) + \", the median is: \" + str(np.median(all_c_sd3))  + \", the maximum is: \" + str(np.max(all_c_sd3)) + \", and the minimum is: \" + str(np.min(all_c_sd3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,7))\n",
    "ax[0].hist(all_c_emd3, bins=30)\n",
    "ax[0].set_title(\"Histogram of Pure Geo (k=97) EMD\")\n",
    "ax[1].hist(all_c_sd3, bins=30)\n",
    "ax[1].set_title(\"Histogram of Pure Geo (k=97) Standard Deviation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower mean but higher median than pre-set neighborhoods for entropy and earth mover's distance with approximately the same number of neighborhoods. We also know that this run has much more evenly sized neighborhoods. So these results make sense. Median says it's a little worse because this method is true random, but mean looks a little bit better because really large neighborhoods don't skew this as far right.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model 1 - Crime Vector Clustering - Use Case for Now\n",
    "\n",
    "### Incorporating features into above geographic clustering algorithm\n",
    "\n",
    "Clustering together areas geographically, and the additional features that weight the clustering are the crime vector/distribution for each cell. This should be our best possible function, we just have to decide whether to use euclidean distance for these additional factors or use earth mover's distance if we find a useful way to do this. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_data = clustering_grid[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "\n",
    "def geo_k_means(data, k, alpha = 0.5, cap=1000, verbose=False):\n",
    "    # pass in data columns that you want to be used for analysis\n",
    "    # store error for iteration analysis\n",
    "    error_mags = []\n",
    "    sse = []\n",
    "    s1 = time.time()\n",
    "    \n",
    "    # initialize centroids\n",
    "    rand_inds = random.sample(range(0, len(data)), k)\n",
    "    arr = data.iloc[rand_inds]\n",
    "    centroids_curr = np.array(arr, dtype=np.float32)\n",
    "    \n",
    "    # centroid storage\n",
    "    centroids_old = np.zeros((k,2))\n",
    "    # cluster label\n",
    "    clusters = np.zeros(len(data))\n",
    "    ideal_error = list(np.zeros(k))\n",
    "    error = []\n",
    "    for i in range(k):\n",
    "        # lats and lons for haversine distance\n",
    "        error.append(hav_dist(centroids_old[i][0], centroids_old[i][1], centroids_curr[i][0], centroids_curr[i][1]))\n",
    "    itera = 0\n",
    "    while not error == ideal_error:\n",
    "        s2 = time.time()\n",
    "        itera += 1\n",
    "        \n",
    "        # stop early\n",
    "        if itera > cap:\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: \" + str(itera))\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            distances = []\n",
    "            lat = data.at[i, 'Latitude']\n",
    "            lon = data.at[i, 'Longitude']\n",
    "            all_c = data.at[i, 'All Crime']\n",
    "            batt = data.at[i, 'Battery']\n",
    "            theft = data.at[i, 'Theft']\n",
    "            narc = data.at[i, 'Narcotics']\n",
    "            for j in range(len(centroids_curr)):\n",
    "                hav = hav_dist(centroids_curr[j][0], centroids_curr[j][1], lat, lon)\n",
    "                curr = np.array([all_c, batt, theft, narc])\n",
    "                cent = np.array([centroids_curr[j][2],centroids_curr[j][3],centroids_curr[j][4],centroids_curr[j][5]])\n",
    "                vec_dis = np.linalg.norm(curr-cent)\n",
    "                distances.append((1.0-alpha) * hav + (alpha) * vec_dis)\n",
    "            cluster_num = np.argmin(distances)\n",
    "            clusters[i] = cluster_num\n",
    "        # Store old centroid values\n",
    "        C_old_store = deepcopy(centroids_curr)\n",
    "        count_duds = 0\n",
    "        data.temp_clust = clusters\n",
    "\n",
    "        sse_iter = 0\n",
    "        for i in range(k):\n",
    "            points_in_clust = data[data.temp_clust == i][['Latitude','Longitude','All Crime','Battery','Theft','Narcotics']]\n",
    "            if len(points_in_clust) > 0:\n",
    "                centroids_curr[i] = np.mean(points_in_clust, axis=0)\n",
    "            else:\n",
    "                count_duds += 1\n",
    "                rand_ind = random.sample(range(0, len(data)), 1)\n",
    "                arr = data.iloc[rand_ind]\n",
    "                centroids_curr[i] = np.array(arr, dtype=np.float32)\n",
    "            mean = centroids_curr[i]\n",
    "            lat = mean[0]\n",
    "            lon = mean[1]\n",
    "            mean_feat = mean[2:]\n",
    "            \n",
    "            matrix = np.array(points_in_clust)\n",
    "            lats = matrix[:,0]\n",
    "            lons = matrix[:,1]\n",
    "            feats = matrix[:,2:]\n",
    "            \n",
    "            for j in range(len(matrix)):\n",
    "                sse_iter = sse_iter + (1.0-alpha)*hav_dist(lat, lon, lats[j], lons[j]) + alpha*np.linalg.norm(mean_feat-feats[j])\n",
    "        sse.append(sse_iter)\n",
    "        error = []\n",
    "        for i in range(k):\n",
    "            hav = hav_dist(C_old_store[i][0], C_old_store[i][1], centroids_curr[i][0], centroids_curr[i][1])\n",
    "            v1 = np.array([C_old_store[i][2], C_old_store[i][3], C_old_store[i][4], C_old_store[i][5]])\n",
    "            v2 = np.array([centroids_curr[i][2], centroids_curr[i][3], centroids_curr[i][4], centroids_curr[i][5]])\n",
    "            vec = np.linalg.norm(v1-v2)\n",
    "            error.append(10.0 * hav + 1.0 * vec)\n",
    "        er = np.linalg.norm(error)\n",
    "        if verbose:\n",
    "            print(\"Magnitude of error: \" + str(er))\n",
    "            print(\"Iteration took: \" + str(time.time()-s2))\n",
    "            print(\"Number of issues: \" + str(count_duds))\n",
    "            print\n",
    "        error_mags.append(er)\n",
    "    print(\"Done, Successful Convergence. Total Time: \" + str(time.time() - s1))\n",
    "    data['CLUSTER_LABEL'] = clusters\n",
    "    return data, error_mags, sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_k_means_pca(data, k, alpha = 0.5, cap=1000, verbose=False):\n",
    "    # pass in data columns that you want to be used for analysis\n",
    "    # store error for iteration analysis\n",
    "    error_mags = []\n",
    "    sse = []\n",
    "    s1 = time.time()\n",
    "    \n",
    "    # initialize centroids\n",
    "    rand_inds = random.sample(range(0, len(data)), k)\n",
    "    arr = data.iloc[rand_inds]\n",
    "    centroids_curr = np.array(arr, dtype=np.float32)\n",
    "    \n",
    "    # centroid storage\n",
    "    centroids_old = np.zeros((k,2))\n",
    "    # cluster label\n",
    "    clusters = np.zeros(len(data))\n",
    "    ideal_error = list(np.zeros(k))\n",
    "    error = []\n",
    "    for i in range(k):\n",
    "        # lats and lons for haversine distance\n",
    "        error.append(hav_dist(centroids_old[i][0], centroids_old[i][1], centroids_curr[i][0], centroids_curr[i][1]))\n",
    "    itera = 0\n",
    "    while not error == ideal_error:\n",
    "        s2 = time.time()\n",
    "        itera += 1\n",
    "        \n",
    "        # stop early\n",
    "        if itera > cap:\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: \" + str(itera))\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            distances = []\n",
    "            lat = data.at[i, 'Latitude']\n",
    "            lon = data.at[i, 'Longitude']\n",
    "            pca_1 = data.at[i, 'PCA_1']\n",
    "            pca_2 = data.at[i, 'PCA_2']\n",
    "            pca_3 = data.at[i, 'PCA_3']\n",
    "            for j in range(len(centroids_curr)):\n",
    "                hav = hav_dist(centroids_curr[j][0], centroids_curr[j][1], lat, lon)\n",
    "                curr = np.array([pca_1, pca_2, pca_3])\n",
    "                cent = np.array([centroids_curr[j][2],centroids_curr[j][3],centroids_curr[j][4]])\n",
    "                vec_dis = np.linalg.norm(curr-cent)\n",
    "                distances.append((1.0-alpha) * hav + (alpha) * vec_dis)\n",
    "            cluster_num = np.argmin(distances)\n",
    "            clusters[i] = cluster_num\n",
    "        # Store old centroid values\n",
    "        C_old_store = deepcopy(centroids_curr)\n",
    "        count_duds = 0\n",
    "        data.temp_clust = clusters\n",
    "\n",
    "        sse_iter = 0\n",
    "        for i in range(k):\n",
    "            points_in_clust = data[data.temp_clust == i][['Latitude','Longitude','PCA_1','PCA_2','PCA_3']]\n",
    "            if len(points_in_clust) > 0:\n",
    "                centroids_curr[i] = np.mean(points_in_clust, axis=0)\n",
    "            else:\n",
    "                count_duds += 1\n",
    "                rand_ind = random.sample(range(0, len(data)), 1)\n",
    "                arr = data.iloc[rand_ind]\n",
    "                centroids_curr[i] = np.array(arr, dtype=np.float32)\n",
    "            mean = centroids_curr[i]\n",
    "            lat = mean[0]\n",
    "            lon = mean[1]\n",
    "            mean_feat = mean[2:]\n",
    "            \n",
    "            matrix = np.array(points_in_clust)\n",
    "            lats = matrix[:,0]\n",
    "            lons = matrix[:,1]\n",
    "            feats = matrix[:,2:]\n",
    "            \n",
    "            for j in range(len(matrix)):\n",
    "                sse_iter = sse_iter + (1.0-alpha)*hav_dist(lat, lon, lats[j], lons[j]) + alpha*np.linalg.norm(mean_feat-feats[j])\n",
    "        sse.append(sse_iter)\n",
    "        error = []\n",
    "        for i in range(k):\n",
    "            hav = hav_dist(C_old_store[i][0], C_old_store[i][1], centroids_curr[i][0], centroids_curr[i][1])\n",
    "            v1 = np.array([C_old_store[i][2], C_old_store[i][3], C_old_store[i][4]])\n",
    "            v2 = np.array([centroids_curr[i][2], centroids_curr[i][3], centroids_curr[i][4]])\n",
    "            vec = np.linalg.norm(v1-v2)\n",
    "            error.append(10.0 * hav + 1.0 * vec)\n",
    "        er = np.linalg.norm(error)\n",
    "        if verbose:\n",
    "            print(\"Magnitude of error: \" + str(er))\n",
    "            print(\"Iteration took: \" + str(time.time()-s2))\n",
    "            print(\"Number of issues: \" + str(count_duds))\n",
    "            print\n",
    "        error_mags.append(er)\n",
    "    print(\"Done, Successful Convergence. Total Time: \" + str(time.time() - s1))\n",
    "    data['CLUSTER_LABEL'] = clusters\n",
    "    return data, error_mags, sse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_k_means_pca_2(data, k, alpha = 0.5, cap=1000, verbose=False):\n",
    "    # pass in data columns that you want to be used for analysis\n",
    "    # store error for iteration analysis\n",
    "    error_mags = []\n",
    "    sse = []\n",
    "    s1 = time.time()\n",
    "    \n",
    "    # initialize centroids\n",
    "    rand_inds = random.sample(range(0, len(data)), k)\n",
    "    arr = data.iloc[rand_inds]\n",
    "    centroids_curr = np.array(arr, dtype=np.float32)\n",
    "    \n",
    "    # centroid storage\n",
    "    centroids_old = np.zeros((k,2))\n",
    "    # cluster label\n",
    "    clusters = np.zeros(len(data))\n",
    "    ideal_error = list(np.zeros(k))\n",
    "    error = []\n",
    "    for i in range(k):\n",
    "        # lats and lons for haversine distance\n",
    "        error.append(hav_dist(centroids_old[i][0], centroids_old[i][1], centroids_curr[i][0], centroids_curr[i][1]))\n",
    "    itera = 0\n",
    "    while not error == ideal_error:\n",
    "        s2 = time.time()\n",
    "        itera += 1\n",
    "        \n",
    "        # stop early\n",
    "        if itera > cap:\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: \" + str(itera))\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            distances = []\n",
    "            lat = data.at[i, 'Latitude']\n",
    "            lon = data.at[i, 'Longitude']\n",
    "            pca_1 = data.at[i, 'PCA_1']\n",
    "            pca_2 = data.at[i, 'PCA_2']\n",
    "            for j in range(len(centroids_curr)):\n",
    "                hav = hav_dist(centroids_curr[j][0], centroids_curr[j][1], lat, lon)\n",
    "                curr = np.array([pca_1, pca_2])\n",
    "                cent = np.array([centroids_curr[j][2],centroids_curr[j][3]])\n",
    "                vec_dis = np.linalg.norm(curr-cent)\n",
    "                distances.append((1.0-alpha) * hav + (alpha) * vec_dis)\n",
    "            cluster_num = np.argmin(distances)\n",
    "            clusters[i] = cluster_num\n",
    "        # Store old centroid values\n",
    "        C_old_store = deepcopy(centroids_curr)\n",
    "        count_duds = 0\n",
    "        data.temp_clust = clusters\n",
    "\n",
    "        sse_iter = 0\n",
    "        for i in range(k):\n",
    "            points_in_clust = data[data.temp_clust == i][['Latitude','Longitude','PCA_1','PCA_2']]\n",
    "            if len(points_in_clust) > 0:\n",
    "                centroids_curr[i] = np.mean(points_in_clust, axis=0)\n",
    "            else:\n",
    "                count_duds += 1\n",
    "                rand_ind = random.sample(range(0, len(data)), 1)\n",
    "                arr = data.iloc[rand_ind]\n",
    "                centroids_curr[i] = np.array(arr, dtype=np.float32)\n",
    "            mean = centroids_curr[i]\n",
    "            lat = mean[0]\n",
    "            lon = mean[1]\n",
    "            mean_feat = mean[2:]\n",
    "            \n",
    "            matrix = np.array(points_in_clust)\n",
    "            lats = matrix[:,0]\n",
    "            lons = matrix[:,1]\n",
    "            feats = matrix[:,2:]\n",
    "            \n",
    "            for j in range(len(matrix)):\n",
    "                sse_iter = sse_iter + (1.0-alpha)*hav_dist(lat, lon, lats[j], lons[j]) + alpha*np.linalg.norm(mean_feat-feats[j])\n",
    "        sse.append(sse_iter)\n",
    "        error = []\n",
    "        for i in range(k):\n",
    "            hav = hav_dist(C_old_store[i][0], C_old_store[i][1], centroids_curr[i][0], centroids_curr[i][1])\n",
    "            v1 = np.array([C_old_store[i][2], C_old_store[i][3]])\n",
    "            v2 = np.array([centroids_curr[i][2], centroids_curr[i][3]])\n",
    "            vec = np.linalg.norm(v1-v2)\n",
    "            error.append(10.0 * hav + 1.0 * vec)\n",
    "        er = np.linalg.norm(error)\n",
    "        if verbose:\n",
    "            print(\"Magnitude of error: \" + str(er))\n",
    "            print(\"Iteration took: \" + str(time.time()-s2))\n",
    "            print(\"Number of issues: \" + str(count_duds))\n",
    "            print\n",
    "        error_mags.append(er)\n",
    "    print(\"Done, Successful Convergence. Total Time: \" + str(time.time() - s1))\n",
    "    data['CLUSTER_LABEL'] = clusters\n",
    "    return data, error_mags, sse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_k_means_pca_1(data, k, alpha = 0.5, cap=1000, verbose=False):\n",
    "    # pass in data columns that you want to be used for analysis\n",
    "    # store error for iteration analysis\n",
    "    error_mags = []\n",
    "    sse = []\n",
    "    s1 = time.time()\n",
    "    \n",
    "    # initialize centroids\n",
    "    rand_inds = random.sample(range(0, len(data)), k)\n",
    "    arr = data.iloc[rand_inds]\n",
    "    centroids_curr = np.array(arr, dtype=np.float32)\n",
    "    \n",
    "    # centroid storage\n",
    "    centroids_old = np.zeros((k,2))\n",
    "    # cluster label\n",
    "    clusters = np.zeros(len(data))\n",
    "    ideal_error = list(np.zeros(k))\n",
    "    error = []\n",
    "    for i in range(k):\n",
    "        # lats and lons for haversine distance\n",
    "        error.append(hav_dist(centroids_old[i][0], centroids_old[i][1], centroids_curr[i][0], centroids_curr[i][1]))\n",
    "    itera = 0\n",
    "    while not error == ideal_error:\n",
    "        s2 = time.time()\n",
    "        itera += 1\n",
    "        \n",
    "        # stop early\n",
    "        if itera > cap:\n",
    "            break\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: \" + str(itera))\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            distances = []\n",
    "            lat = data.at[i, 'Latitude']\n",
    "            lon = data.at[i, 'Longitude']\n",
    "            pca_1 = data.at[i, 'PCA_1']\n",
    "            for j in range(len(centroids_curr)):\n",
    "                hav = hav_dist(centroids_curr[j][0], centroids_curr[j][1], lat, lon)\n",
    "                curr = np.array([pca_1])\n",
    "                cent = np.array([centroids_curr[j][2]])\n",
    "                vec_dis = np.linalg.norm(curr-cent)\n",
    "                distances.append((1.0-alpha) * hav + (alpha) * vec_dis)\n",
    "            cluster_num = np.argmin(distances)\n",
    "            clusters[i] = cluster_num\n",
    "        # Store old centroid values\n",
    "        C_old_store = deepcopy(centroids_curr)\n",
    "        count_duds = 0\n",
    "        data.temp_clust = clusters\n",
    "\n",
    "        sse_iter = 0\n",
    "        for i in range(k):\n",
    "            points_in_clust = data[data.temp_clust == i][['Latitude','Longitude','PCA_1']]\n",
    "            if len(points_in_clust) > 0:\n",
    "                centroids_curr[i] = np.mean(points_in_clust, axis=0)\n",
    "            else:\n",
    "                count_duds += 1\n",
    "                rand_ind = random.sample(range(0, len(data)), 1)\n",
    "                arr = data.iloc[rand_ind]\n",
    "                centroids_curr[i] = np.array(arr, dtype=np.float32)\n",
    "            mean = centroids_curr[i]\n",
    "            lat = mean[0]\n",
    "            lon = mean[1]\n",
    "            mean_feat = mean[2:]\n",
    "            \n",
    "            matrix = np.array(points_in_clust)\n",
    "            lats = matrix[:,0]\n",
    "            lons = matrix[:,1]\n",
    "            feats = matrix[:,2:]\n",
    "            \n",
    "            for j in range(len(matrix)):\n",
    "                sse_iter = sse_iter + (1.0-alpha)*hav_dist(lat, lon, lats[j], lons[j]) + alpha*np.linalg.norm(mean_feat-feats[j])\n",
    "        sse.append(sse_iter)\n",
    "        error = []\n",
    "        for i in range(k):\n",
    "            hav = hav_dist(C_old_store[i][0], C_old_store[i][1], centroids_curr[i][0], centroids_curr[i][1])\n",
    "            v1 = np.array([C_old_store[i][2]])\n",
    "            v2 = np.array([centroids_curr[i][2]])\n",
    "            vec = np.linalg.norm(v1-v2)\n",
    "            error.append(10.0 * hav + 1.0 * vec)\n",
    "        er = np.linalg.norm(error)\n",
    "        if verbose:\n",
    "            print(\"Magnitude of error: \" + str(er))\n",
    "            print(\"Iteration took: \" + str(time.time()-s2))\n",
    "            print(\"Number of issues: \" + str(count_duds))\n",
    "            print\n",
    "        error_mags.append(er)\n",
    "    print(\"Done, Successful Convergence. Total Time: \" + str(time.time() - s1))\n",
    "    data['CLUSTER_LABEL'] = clusters\n",
    "    return data, error_mags, sse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d5, error_mags5, sse5 = geo_k_means(clustering_data, 97, cap=80, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,14))\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d5, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_grid['FEATURE_CLUSTER_LABEL'] = [int(i) for i in list(new_d5.CLUSTER_LABEL)]\n",
    "all_c_emd4 = emd(clustering_grid, 'FEATURE_CLUSTER_LABEL')\n",
    "print(\"The average Simple Cluster all crime EMD is: \" + str(np.mean(all_c_emd4)) + \", the median is: \" + str(np.median(all_c_emd4)) + \", the maximum is: \" + str(np.max(all_c_emd4)) + \", and the minimum is: \" + str(np.min(all_c_emd4)))\n",
    "print\n",
    "print('----------------------------------------------------------------------------')\n",
    "print\n",
    "all_c_sd4 = sd(clustering_grid, 'FEATURE_CLUSTER_LABEL')\n",
    "print(\"The average Simple Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd4)) + \", the median is: \" + str(np.median(all_c_sd4))  + \", the maximum is: \" + str(np.max(all_c_sd4)) + \", and the minimum is: \" + str(np.min(all_c_sd4)))\n",
    "print\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].plot(range(len(error_mags5)), error_mags5)\n",
    "axs[0].set_xlabel('Iteration Number')\n",
    "axs[0].set_ylabel('Magnitude of Iteration Error Vector')\n",
    "axs[0].set_title('Convergence Analysis (Using Movement of Centroids)')\n",
    "axs[1].plot(range(len(sse5)), sse5)\n",
    "axs[1].set_xlabel('Iteration Number')\n",
    "axs[1].set_ylabel('Magnitude of Iteration SSE')\n",
    "axs[1].set_title('Convergence Analysis (SSE, Should Decrease)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing these results (on 97 clusters, same as Neighborhood baseline):\n",
    "\n",
    "Great results, super homogeneous! Standard Deviation doesn't show the difference as well, although it looks decent, but EMD really shows how good this \"optimal\" model is, even though it really doesn't represent neighborhoods. This is what we want to see because it really should prioritize clustering areas with similar crime data, thus crime homogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Normalize data before running 'geo_k_means'\n",
    "\n",
    "#### Attempt 1: Normalize Everything\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_norm = deepcopy(clustering_data)[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "normalized_clustering_data = (data_for_norm-data_for_norm.mean())/data_for_norm.std()[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "new_d7, error_mags7, sse7 = geo_k_means(normalized_clustering_data, 97, cap=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "plt.figure(figsize=(10,14))\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d7, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "clustering_grid['NORM_FEATURE_CLUSTER_LABEL'] = [int(i) for i in list(new_d7.CLUSTER_LABEL)]\n",
    "all_c_emd5 = emd(clustering_grid, 'NORM_FEATURE_CLUSTER_LABEL')\n",
    "all_c_sd5 = sd(clustering_grid, 'NORM_FEATURE_CLUSTER_LABEL')\n",
    "print(\"The average Simple Cluster all crime EMD is: \" + str(np.mean(all_c_emd5)) + \", the median is: \" + str(np.median(all_c_emd5)) + \", the maximum is: \" + str(np.max(all_c_emd5)) + \", and the minimum is: \" + str(np.min(all_c_emd5)))\n",
    "print\n",
    "print('-----------------------------------------------------')\n",
    "print\n",
    "print(\"The average Simple Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd5)) + \", the median is: \" + str(np.median(all_c_sd5)) + \", the maximum is: \" + str(np.max(all_c_sd5)) + \", and the minimum is: \" + str(np.min(all_c_sd5)))\n",
    "print\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].plot(range(len(error_mags7)), error_mags7)\n",
    "axs[0].set_xlabel('Iteration Number')\n",
    "axs[0].set_ylabel('Magnitude of Iteration Error Vector')\n",
    "axs[0].set_title('Convergence Analysis (Using Movement of Centroids)')\n",
    "axs[1].plot(range(len(sse7)), sse7)\n",
    "axs[1].set_xlabel('Iteration Number')\n",
    "axs[1].set_ylabel('Magnitude of Iteration SSE')\n",
    "axs[1].set_title('Convergence Analysis (SSE, Should Decrease)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 2: Normalize all but Lat and Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_norm = deepcopy(clustering_data)[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "\n",
    "for col in ['All Crime', 'Battery', 'Theft', 'Narcotics']:\n",
    "    data_for_norm[col] = (data_for_norm[col]-data_for_norm[col].mean())/data_for_norm[col].std()\n",
    "new_d8, error_mags8, sse8 = geo_k_means(data_for_norm, 97, cap=40, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d8, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "clustering_grid['SEMI_NORM_FEATURE_CLUSTER_LABEL'] = [int(i) for i in list(new_d8.CLUSTER_LABEL)]\n",
    "all_c_emd6 = emd(clustering_grid, 'SEMI_NORM_FEATURE_CLUSTER_LABEL')\n",
    "all_c_sd6 = sd(clustering_grid, 'SEMI_NORM_FEATURE_CLUSTER_LABEL')\n",
    "print(\"The average Simple Cluster all crime EMD is: \" + str(np.mean(all_c_emd6)) + \", the median is: \" + str(np.median(all_c_emd6)) + \", the maximum is: \" + str(np.max(all_c_emd6)) + \", and the minimum is: \" + str(np.min(all_c_emd6)))\n",
    "print\n",
    "print('-----------------------------------------------------')\n",
    "print\n",
    "print(\"The average Simple Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd6)) + \", the median is: \" + str(np.median(all_c_sd6)) + \", the maximum is: \" + str(np.max(all_c_sd6)) + \", and the minimum is: \" + str(np.min(all_c_sd6)))\n",
    "print\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].plot(range(len(error_mags8)), error_mags8)\n",
    "axs[0].set_xlabel('Iteration Number')\n",
    "axs[0].set_ylabel('Magnitude of Iteration Error Vector')\n",
    "axs[0].set_title('Convergence Analysis (Using Movement of Centroids)')\n",
    "axs[1].plot(range(len(sse8)), sse8)\n",
    "axs[1].set_xlabel('Iteration Number')\n",
    "axs[1].set_ylabel('Magnitude of Iteration SSE')\n",
    "axs[1].set_title('Convergence Analysis (SSE, Should Decrease)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting. EMD looks considerably better for this model, entropy looks pretty bad. Not exactly sure why this is, but I think it makes sense that the result should be more homogeneous than the baseline because it considers the data. It's starting to seem like there is overwhelming evidence that EMD is a better metric of homogeneity (and it is a topic people will want to see in writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,7))\n",
    "ax[0].hist(all_c_emd6, bins=30)\n",
    "ax[0].set_title(\"Histogram of Semi-Normalized Run EMD\")\n",
    "ax[1].hist(all_c_sd6, bins=30)\n",
    "ax[1].set_title(\"Histogram of Semi-Normalized Run Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of these models:\n",
    "\n",
    "As noted by the fact that model 2 is run, model 1 doesn't really make sense because haversine distance is specifically for lat/lon, not for a transformed variable. Model 2 is tricky because it doesn't weight hav distance and vector distance properly but it doesn't blatantly handle distance incorrectly. Visually, it actually looks pretty solid because it clearly considers the data and warps neighborhood boundaries, but they still look reasonably like neighborhood boundaries in. Once we tune alpha, this will look better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Trying Models Again With PCA Data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First with All Raw Columns - First 3 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data_2 = copy.deepcopy(clustering_grid[['Latitude', 'Longitude', 'PCA_1', 'PCA_2', 'PCA_3']])\n",
    "new_d12, error_mags12, sse12 = geo_k_means_pca(cluster_data_2, 97, cap=80, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d12, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "clustering_grid['PCA_FEATURES_CLUSTERS'] = [int(i) for i in list(new_d12.CLUSTER_LABEL)]\n",
    "all_c_emd12 = emd(clustering_grid, 'PCA_FEATURES_CLUSTERS')\n",
    "all_c_sd12 = sd(clustering_grid, 'PCA_FEATURES_CLUSTERS')\n",
    "print(\"The average Simple Cluster all crime EMD is: \" + str(np.mean(all_c_emd12)) + \", the median is: \" + str(np.median(all_c_emd12)) + \", the maximum is: \" + str(np.max(all_c_emd12)) + \", and the minimum is: \" + str(np.min(all_c_emd12)))\n",
    "print\n",
    "print('-----------------------------------------------------')\n",
    "print\n",
    "print(\"The average Simple Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd12)) + \", the median is: \" + str(np.median(all_c_sd12)) + \", the maximum is: \" + str(np.max(all_c_sd12)) + \", and the minimum is: \" + str(np.min(all_c_sd12)))\n",
    "print\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].plot(range(len(error_mags12)), error_mags12)\n",
    "axs[0].set_xlabel('Iteration Number')\n",
    "axs[0].set_ylabel('Magnitude of Iteration Error Vector')\n",
    "axs[0].set_title('Convergence Analysis (Using Movement of Centroids)')\n",
    "axs[1].plot(range(len(sse12)), sse12)\n",
    "axs[1].set_xlabel('Iteration Number')\n",
    "axs[1].set_ylabel('Magnitude of Iteration SSE')\n",
    "axs[1].set_title('Convergence Analysis (SSE, Should Decrease)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next with Normalized PCA Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_norm_data = copy.deepcopy(clustering_grid[['Latitude', 'Longitude', 'PCA_1', 'PCA_2', 'PCA_3']])\n",
    "for col in ['PCA_1', 'PCA_2', 'PCA_3']:\n",
    "    pca_norm_data[col] = (pca_norm_data[col]-pca_norm_data[col].mean())/pca_norm_data[col].std()\n",
    "new_d13, error_mags13, sse13 = geo_k_means_pca(pca_norm_data, 97, cap=80, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=new_d13, fit_reg=False, hue='CLUSTER_LABEL', legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "clustering_grid['PCA_FEATURES_NORM_CLUSTERS'] = [int(i) for i in list(new_d13.CLUSTER_LABEL)]\n",
    "all_c_emd13 = emd(clustering_grid, 'PCA_FEATURES_NORM_CLUSTERS')\n",
    "all_c_sd13 = sd(clustering_grid, 'PCA_FEATURES_NORM_CLUSTERS')\n",
    "print(\"The average Simple Cluster all crime EMD is: \" + str(np.mean(all_c_emd13)) + \", the median is: \" + str(np.median(all_c_emd13)) + \", the maximum is: \" + str(np.max(all_c_emd13)) + \", and the minimum is: \" + str(np.min(all_c_emd13)))\n",
    "print\n",
    "print('-----------------------------------------------------')\n",
    "print\n",
    "print(\"The average Simple Cluster all crime Standard Deviation is: \" + str(np.mean(all_c_sd13)) + \", the median is: \" + str(np.median(all_c_sd13)) + \", the maximum is: \" + str(np.max(all_c_sd13)) + \", and the minimum is: \" + str(np.min(all_c_sd13)))\n",
    "print\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "axs[0].plot(range(len(error_mags13)), error_mags13)\n",
    "axs[0].set_xlabel('Iteration Number')\n",
    "axs[0].set_ylabel('Magnitude of Iteration Error Vector')\n",
    "axs[0].set_title('Convergence Analysis (Using Movement of Centroids)')\n",
    "axs[1].plot(range(len(sse13)), sse13)\n",
    "axs[1].set_xlabel('Iteration Number')\n",
    "axs[1].set_ylabel('Magnitude of Iteration SSE')\n",
    "axs[1].set_title('Convergence Analysis (SSE, Should Decrease)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduce a Regularization Parameter (probably needs more work)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research on Regularization to write up:\n",
    "\n",
    "We could count the number of distinct regions. Number of disjoint regions\n",
    "\n",
    "For each cell, we can count up how many neighboring cells are in the same neighborhood and add up total number of non-neighbor relationships. \n",
    "\n",
    "An internal neighborhood cell contributes 0\n",
    "\n",
    "A regular border cell contibutes 3 (center one)\n",
    "\n",
    "XXX\n",
    "OOO\n",
    "OOO\n",
    "\n",
    "Contributing 4\n",
    "\n",
    "XXX\n",
    "OOX\n",
    "OOO\n",
    "\n",
    "And so on...\n",
    "\n",
    "A completely isolated cell contribues 8\n",
    "\n",
    "XXX\n",
    "XOX\n",
    "XXX\n",
    "\n",
    "Logically, this seems to satisfy the conditions that we want to penalize. Jaggedness and isolation especially\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(data, cluster_col):\n",
    "    \n",
    "    s = time.time()\n",
    "    lats = set(list(data.Latitude))\n",
    "    lons = set(list(data.Longitude))\n",
    "    \n",
    "    #total_existing = 0\n",
    "    total_reg_penalty = 0\n",
    "    #failures = 0\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        lat = row.Latitude\n",
    "        lon = row.Longitude\n",
    "        nei_identity = row[cluster_col]\n",
    "        \n",
    "        s1 = time.time()\n",
    "        \n",
    "        try:\n",
    "            lat_above = np.min([i for i in lats if i > lat])\n",
    "        except:\n",
    "            #failures = failures + 1\n",
    "            lat_above = 0.0\n",
    "        try:\n",
    "            lat_below = np.max([i for i in lats if i < lat])\n",
    "        except:\n",
    "            #failures = failures + 1\n",
    "            lat_below = 0.0\n",
    "        try:\n",
    "            lon_above = np.min([i for i in lons if i > lon])\n",
    "        except:\n",
    "            #failures = failures + 1\n",
    "            lon_above = 0.0\n",
    "        try:\n",
    "            lon_below = np.max([i for i in lons if i < lon])\n",
    "        except:\n",
    "            #failures = failures + 1\n",
    "            lon_below = 0.0\n",
    "            \n",
    "        s2 = time.time()\n",
    "        \n",
    "        lat_list = [i for i in [lat, lat_above, lat_below] if i != 0.0]\n",
    "        lon_list = [i for i in [lon, lon_above, lon_below] if i != 0.0]\n",
    "        \n",
    "        for latitude in lat_list:\n",
    "            lat_cut = data[data.Latitude == latitude]\n",
    "            for longitude in lon_list:\n",
    "                #allegiance = data.loc[(data.Latitude == latitude) & (data.Longitude == longitude)][cluster_col]\n",
    "                lon_cut = lat_cut[lat_cut.Longitude == longitude]\n",
    "                allegiance = list(lon_cut[cluster_col])\n",
    "                \n",
    "                if len(allegiance) == 1:\n",
    "                    #total_existing = total_existing + 1\n",
    "                    if not allegiance[0] == nei_identity:\n",
    "                        total_reg_penalty = total_reg_penalty + 1\n",
    "        \n",
    "        s3 = time.time()\n",
    "        \n",
    "    print(\"Total time: \" + str(time.time() - s))\n",
    "    print(\"Regularization Penalty: \" + str(total_reg_penalty))\n",
    "        \n",
    "    return total_reg_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Beats should probably be greater than neighborhoods because there are so many different areas. It seems to work! Regularization takes longer than we want (~20-25 seconds for each run), but this probably won't be prohibitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running for 97 Neighborhoods\")\n",
    "nei_reg = regularize(clustering_grid, 'Neighborhoods')\n",
    "print\n",
    "print(\"Running for 260+ Beats\")\n",
    "beat_reg = regularize(clustering_grid, 'Beats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Set up elbow plot and silhouette plot\n",
    "\n",
    "### First on one run\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Method\n",
    "\n",
    "The silhouette measures the difference between the dissimilarity between observation $i$ and other points in the next clostest cluster and other points in the cluster in which $i$ belongs to. Here we look at the average silhouette statistic across clusters. It is intuitive that we want to *maximize* this value. \n",
    "\n",
    "For this example, we will stick with calculating distance as Euclidean distance for simplicity and for the sake of run time. Given what we have seen in comparing euclidean distance to Haversine distance, this should still be useful.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def silhouette(data, cluster_col, n_clusters):\n",
    "    cols = ['Latitude', 'Longitude', 'All Crime', 'Battery', 'Assault', 'Theft']\n",
    "    X = data[cols]\n",
    "    cluster_labels = data[cluster_col]\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters=\" + str(n_clusters) + \" The average silhouette_score is: \"+ str(silhouette_avg))\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.show()\n",
    "    \n",
    "    return silhouette_avg\n",
    "    \n",
    "print(\"Neighborhood silhouette analysis\")\n",
    "silhouette(clustering_grid, 'Neighborhoods', 97)\n",
    "print\n",
    "print(\"For a better previously run model\")\n",
    "silhouette(clustering_grid, 'SEMI_NORM_FEATURE_CLUSTER_LABEL', 97)\n",
    "print\n",
    "print(\"For a totally unrealistic but highly homogeneous model\")\n",
    "silhouette(clustering_grid, 'FEATURE_CLUSTER_LABEL', 97)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis*: This seems to get into our question abnout optimal value of alpha for weighting features vs. geography but could be a good way to compare k values when we hold alpha constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gap Statistic\n",
    "\n",
    "The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e, that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\n",
    "\n",
    "Python Implementation: \n",
    "\n",
    "https://snippets.cacher.io/snippet/992c45f7b11bacc5e862\n",
    "\n",
    "If I implement it myself, go off this: https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/#gap-statistic-method but I am confused about what it means by generate a uniform random distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not Really relevant to this project upon further review, so silhouette plot and elbow plot will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on Lots of K Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [80, 120]#[80, 85, 90, 95, 100, 105, 110, 115, 120]\n",
    "\n",
    "data_for_norm3 = deepcopy(clustering_data)[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "for col in ['All Crime', 'Battery', 'Theft', 'Narcotics']:\n",
    "    data_for_norm3[col] = (data_for_norm3[col]-data_for_norm3[col].mean())/data_for_norm3[col].std()\n",
    "#clustering_data = clustering_grid[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "\n",
    "sd_list_3 = []\n",
    "emd_list_3 = []\n",
    "reg_pen_list_3 = []\n",
    "final_sse = []\n",
    "silhouettes = []\n",
    "\n",
    "for k_val in k_list:\n",
    "    \n",
    "    df, error, sse = geo_k_means(data_for_norm3[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']], k=k_val, alpha=0.3, cap=1,verbose=False)\n",
    "    #df, error = pure_geo_k_means_capped(clustering_data[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']], k_val, 15)\n",
    "    str_lab = \"Cluster_Label_K_\" + str(k_val)\n",
    "    clustering_grid[str_lab] = [int(i) for i in list(df.CLUSTER_LABEL)]\n",
    "    \n",
    "    sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=str_lab, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "    plt.show()\n",
    "    \n",
    "    reg_pen3 = regularize(clustering_grid, str_lab)\n",
    "    reg_pen_list_3.append(reg_pen3)\n",
    "    final_sse.append(sse[len(sse)-1])\n",
    "    \n",
    "    all_c_emd = emd(clustering_grid, str_lab)\n",
    "    emd_list_3.append(np.median(all_c_emd))\n",
    "    all_c_sd = sd(clustering_grid, str_lab)\n",
    "    sd_list_3.append(np.median(all_c_sd))\n",
    "    sil_val = silhouette(clustering_grid, str_lab, k_val)\n",
    "    silhouettes.append(sil_val)\n",
    "    print(\"K Value: \" + str(k_val) + \", median EMD: \" + str(np.median(all_c_emd)))\n",
    "    print\n",
    "    \n",
    "fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "axs[0].plot(k_list, final_sse)\n",
    "#Regularization = plt.plot(k_list, [x for x in reg_pen_list_3])\n",
    "axs[0].set_xlabel(\"K Value\")\n",
    "axs[0].set_ylabel(\"Average All SSE\")\n",
    "axs[0].set_title(\"K-Means Clustering Elbow Plot\")\n",
    "axs[1].plot(k_list, silhouettes)\n",
    "axs[1].set_xlabel(\"K Value\")\n",
    "axs[1].set_ylabel(\"Average Silhouette\")\n",
    "axs[1].set_title(\"Silhouette Method for Optimal K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silhouettes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_k_run(alpha):\n",
    "    k_list = [80, 85, 90, 95, 100, 105, 110, 115, 120]\n",
    "\n",
    "    data_for_norm3 = deepcopy(clustering_data)[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "    for col in ['All Crime', 'Battery', 'Theft', 'Narcotics']:\n",
    "        data_for_norm3[col] = (data_for_norm3[col]-data_for_norm3[col].mean())/data_for_norm3[col].std()\n",
    "    #clustering_data = clustering_grid[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "\n",
    "    sd_list_3 = []\n",
    "    emd_list_3 = []\n",
    "    reg_pen_list_3 = []\n",
    "    final_sse = []\n",
    "    silhouettes = []\n",
    "\n",
    "    for k_val in k_list:\n",
    "    \n",
    "        df, error, sse = geo_k_means(data_for_norm3[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']], k=k_val, alpha=alpha, cap=1000,verbose=False)\n",
    "        #df, error = pure_geo_k_means_capped(clustering_data[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']], k_val, 15)\n",
    "        str_lab = \"Cluster_Label_K_\" + str(k_val)\n",
    "        clustering_grid[str_lab] = [int(i) for i in list(df.CLUSTER_LABEL)]\n",
    "    \n",
    "        sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=str_lab, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "        plt.show()\n",
    "    \n",
    "        reg_pen3 = regularize(clustering_grid, str_lab)\n",
    "        reg_pen_list_3.append(reg_pen3)\n",
    "        final_sse.append(sse[len(sse)-1])\n",
    "    \n",
    "        all_c_emd = emd(clustering_grid, str_lab)\n",
    "        emd_list_3.append(np.median(all_c_emd))\n",
    "        all_c_sd = sd(clustering_grid, str_lab)\n",
    "        sd_list_3.append(np.median(all_c_sd))\n",
    "        sil_val = silhouette(clustering_grid, str_lab, k_val)\n",
    "        silhouettes.append(sil_val)\n",
    "        print(\"K Value: \" + str(k_val) + \", median EMD: \" + str(np.median(all_c_emd)))\n",
    "        print\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "    axs[0].plot(k_list, final_sse)\n",
    "    #Regularization = plt.plot(k_list, [x for x in reg_pen_list_3])\n",
    "    axs[0].set_xlabel(\"K Value\")\n",
    "    axs[0].set_ylabel(\"Average All SSE\")\n",
    "    axs[0].set_title(\"K-Means Clustering Elbow Plot\")\n",
    "    axs[1].plot(k_list, silhouettes)\n",
    "    axs[1].set_xlabel(\"K Value\")\n",
    "    axs[1].set_ylabel(\"Average Silhouette\")\n",
    "    axs[1].set_title(\"Silhouette Method for Optimal K\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big run to find optimal K for each alpha!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Takes a while to Run- Don't Re-run if Avoidable !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "for alp in alphas:\n",
    "    print(\"RUNNING NOW FOR ALPHA=\" + str(alp))\n",
    "    optimal_k_run(alp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from above:\n",
    "\n",
    "<img src=\"Files/0.01.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.01, k = 105?\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.1.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.1, k = 110\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.2.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.2, k = 100\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.3.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.3, k = 90\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.4.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.4, k = 100\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.5.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.5, k = 85\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.6.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.6, k = 90\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.7.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.7, k = 85\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.8.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.8, k = 100\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.9.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.9, k = 85\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"Files/0.99.png\" width=\"700\">\n",
    "\n",
    "Alpha = 0.99, k = 95\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run to Optimize $\\alpha$ Using Regular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_run_list = [(0.01,105),(0.1,110),(0.2,100),(0.3,90),(0.4,100),(0.5,85),(0.6,90),(0.7,85),(0.8,100),(0.9,85),(0.99,95)]\n",
    "    \n",
    "data_for_norm5 = deepcopy(clustering_data)[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "for col in ['All Crime', 'Battery', 'Theft', 'Narcotics']:\n",
    "    data_for_norm5[col] = (data_for_norm5[col]-data_for_norm5[col].mean())/data_for_norm5[col].std()\n",
    "\n",
    "emd_list_5 = []\n",
    "reg_pen_list_5 = []\n",
    "\n",
    "for alp, k_val in optimal_run_list:\n",
    "    df, error, sse5 = geo_k_means(data_for_norm5[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']], k_val, alpha=alp, cap=1000,verbose=False)\n",
    "    str_lab = \"Cluster_Label_A2_\" + str(alp)\n",
    "    clustering_grid[str_lab] = [int(i) for i in list(df.CLUSTER_LABEL)]\n",
    "    \n",
    "    sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=str_lab, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "    plt.show()\n",
    "    \n",
    "    reg_pen5 = regularize(clustering_grid, str_lab)\n",
    "    reg_pen_list_5.append(reg_pen5)\n",
    "    \n",
    "    all_c_emd = emd(clustering_grid, str_lab)\n",
    "    emd_list_5.append(np.median(all_c_emd))\n",
    "    print(\"Alpha: \" + str(alp) + \", K: \" + str(k_val) + \", median EMD: \" + str(np.median(all_c_emd)))\n",
    "    print                                                                       \n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "axs[0].plot(alphas, emd_list_5)\n",
    "axs[0].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[0].set_ylabel(\"Average All Crime EMD\")\n",
    "axs[0].set_title(\"EMD vs. Alpha for Model Selection\")\n",
    "axs[1].plot(alphas, reg_pen_list_5)\n",
    "axs[1].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[1].set_ylabel(\"Average Regularization Penalty\")\n",
    "axs[1].set_title(\"Regularization vs. Alpha for Model Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptable range is 0.4-0.7. Before 0.4, EMD is consistent so no confidence that we're improving beyond the baseline. After 0.7, regularization penalty skyrockets, and we can see in the maps that they start to get very random and don't look like real neighborhoods. Looking at the maps, I think alpha = 0.4, k=100 is a good map to choose, and it seems like we could use the idea of the regularization again to smooth out areas where there is a contribution of 7 or 8 to the regularization and re-assign these points to one of the neighboring clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run to Optimize $\\alpha$ Using 3 PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use 100 since this is a test run to see what happens\n",
    "optimal_run_list = [(0.01,100),(0.1,100),(0.2,100),(0.3,100),(0.4,100),(0.5,100),(0.6,100),(0.7,100),(0.8,100),(0.9,100),(0.99,100)]\n",
    "    \n",
    "data_for_norm15 = deepcopy(clustering_grid)[['Latitude', 'Longitude', 'PCA_1', 'PCA_2', 'PCA_3']]\n",
    "for col in ['PCA_1', 'PCA_2', 'PCA_3']:\n",
    "    data_for_norm15[col] = (data_for_norm15[col]-data_for_norm15[col].mean())/data_for_norm15[col].std()\n",
    "\n",
    "emd_list_15 = []\n",
    "reg_pen_list_15 = []\n",
    "\n",
    "for alp, k_val in optimal_run_list:\n",
    "    df, error, sse15 = geo_k_means_pca(data_for_norm15[['Latitude', 'Longitude', 'PCA_1', 'PCA_2', 'PCA_3']], k_val, alpha=alp, cap=1000,verbose=False)\n",
    "    str_lab = \"Cluster_Label_A3_\" + str(alp)\n",
    "    clustering_grid[str_lab] = [int(i) for i in list(df.CLUSTER_LABEL)]\n",
    "    \n",
    "    sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=str_lab, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "    plt.show()\n",
    "    \n",
    "    reg_pen15 = regularize(clustering_grid, str_lab)\n",
    "    reg_pen_list_15.append(reg_pen15)\n",
    "    \n",
    "    all_c_emd = emd(clustering_grid, str_lab)\n",
    "    emd_list_15.append(np.median(all_c_emd))\n",
    "    print(\"Alpha: \" + str(alp) + \", K: \" + str(k_val) + \", median EMD: \" + str(np.median(all_c_emd)))\n",
    "    print                                                                       \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "axs[0].plot(alphas, emd_list_15)\n",
    "axs[0].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[0].set_ylabel(\"Average All Crime EMD\")\n",
    "axs[0].set_title(\"EMD vs. Alpha for Model Selection\")\n",
    "axs[1].plot(alphas, reg_pen_list_15)\n",
    "axs[1].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[1].set_ylabel(\"Average Regularization Penalty\")\n",
    "axs[1].set_title(\"Regularization vs. Alpha for Model Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run to Optimize $\\alpha$ Using 2 PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use 100 since this is a test run to see what happens\n",
    "optimal_run_list = [(0.01,100),(0.1,100),(0.2,100),(0.3,100),(0.4,100),(0.5,100),(0.6,100),(0.7,100),(0.8,100),(0.9,100),(0.99,100)]\n",
    "    \n",
    "data_for_norm16 = deepcopy(clustering_grid)[['Latitude', 'Longitude', 'PCA_1', 'PCA_2']]\n",
    "for col in ['PCA_1', 'PCA_2']:\n",
    "    data_for_norm16[col] = (data_for_norm16[col]-data_for_norm16[col].mean())/data_for_norm16[col].std()\n",
    "\n",
    "emd_list_16 = []\n",
    "reg_pen_list_16 = []\n",
    "\n",
    "for alp, k_val in optimal_run_list:\n",
    "    df, error, sse16 = geo_k_means_pca_2(data_for_norm16[['Latitude', 'Longitude', 'PCA_1', 'PCA_2']], k_val, alpha=alp, cap=1000,verbose=False)\n",
    "    str_lab = \"Cluster_Label_A4_\" + str(alp)\n",
    "    clustering_grid[str_lab] = [int(i) for i in list(df.CLUSTER_LABEL)]\n",
    "    \n",
    "    sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=str_lab, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "    plt.show()\n",
    "    \n",
    "    reg_pen16 = regularize(clustering_grid, str_lab)\n",
    "    reg_pen_list_16.append(reg_pen16)\n",
    "    \n",
    "    all_c_emd = emd(clustering_grid, str_lab)\n",
    "    emd_list_16.append(np.median(all_c_emd))\n",
    "    print(\"Alpha: \" + str(alp) + \", K: \" + str(k_val) + \", median EMD: \" + str(np.median(all_c_emd)))\n",
    "    print    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "axs[0].plot(alphas, emd_list_16)\n",
    "axs[0].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[0].set_ylabel(\"Average All Crime EMD\")\n",
    "axs[0].set_title(\"EMD vs. Alpha for Model Selection\")\n",
    "axs[1].plot(alphas, reg_pen_list_16)\n",
    "axs[1].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[1].set_ylabel(\"Average Regularization Penalty\")\n",
    "axs[1].set_title(\"Regularization vs. Alpha for Model Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run to Optimize $\\alpha$ Using 1 PCA Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use 100 since this is a test run to see what happens\n",
    "optimal_run_list = [(0.01,100),(0.1,100),(0.2,100),(0.3,100),(0.4,100),(0.5,100),(0.6,100),(0.7,100),(0.8,100),(0.9,100),(0.99,100)]\n",
    "    \n",
    "data_for_norm17 = deepcopy(clustering_grid)[['Latitude', 'Longitude', 'PCA_1']]\n",
    "for col in ['PCA_1']:\n",
    "    data_for_norm17[col] = (data_for_norm17[col]-data_for_norm17[col].mean())/data_for_norm17[col].std()\n",
    "\n",
    "emd_list_17 = []\n",
    "reg_pen_list_17 = []\n",
    "\n",
    "for alp, k_val in optimal_run_list:\n",
    "    df, error, sse17 = geo_k_means_pca_1(data_for_norm17[['Latitude', 'Longitude', 'PCA_1']], k_val, alpha=alp, cap=1000,verbose=False)\n",
    "    str_lab = \"Cluster_Label_A5_\" + str(alp)\n",
    "    clustering_grid[str_lab] = [int(i) for i in list(df.CLUSTER_LABEL)]\n",
    "    \n",
    "    sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=str_lab, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "    plt.show()\n",
    "    \n",
    "    reg_pen17 = regularize(clustering_grid, str_lab)\n",
    "    reg_pen_list_17.append(reg_pen17)\n",
    "    \n",
    "    all_c_emd = emd(clustering_grid, str_lab)\n",
    "    emd_list_17.append(np.median(all_c_emd))\n",
    "    print(\"Alpha: \" + str(alp) + \", K: \" + str(k_val) + \", median EMD: \" + str(np.median(all_c_emd)))\n",
    "    print    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "axs[0].plot(alphas, emd_list_17)\n",
    "axs[0].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[0].set_ylabel(\"Average All Crime EMD\")\n",
    "axs[0].set_title(\"EMD vs. Alpha for Model Selection\")\n",
    "axs[1].plot(alphas, reg_pen_list_17)\n",
    "axs[1].set_xlabel(\"Alpha (with Corresponding Optimal K)\")\n",
    "axs[1].set_ylabel(\"Average Regularization Penalty\")\n",
    "axs[1].set_title(\"Regularization vs. Alpha for Model Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's what we're trying to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=\"Cluster_Label_A2_0.4\"\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=label, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "# Get baseline\n",
    "print(\"Number of Neighborhoods: \" + str(len(np.unique(list(clustering_grid[label])))))\n",
    "all_c_emd6 = emd(clustering_grid[clustering_grid[label] > -1], label)\n",
    "all_c_sd6 = sd(clustering_grid[clustering_grid[label] > -1], label)\n",
    "print(\"The average neighborhood all crime EMD is: \" + str(np.mean(all_c_emd6)) + \", the median is: \" + str(np.median(all_c_emd6)) + \", the maximum is: \" + str(np.max(all_c_emd6)) + \", and the minimum is: \" + str(np.min(all_c_emd6)))\n",
    "print(\"The average neighborhood all crime Standard Deviation is: \" + str(np.mean(all_c_sd6)) + \", the median is: \" + str(np.median(all_c_sd6)) + \", the maximum is: \" + str(np.max(all_c_sd6)) + \", and the minimum is: \" + str(np.min(all_c_sd6)))\n",
    "print(\"The regularization parameter is \" + str(reg_pen_list_5[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "lats = set(list(clustering_grid.Latitude))\n",
    "lons = set(list(clustering_grid.Longitude))\n",
    "\n",
    "new_label = label + \"_Fixed\"\n",
    "clustering_grid[new_label] = list(clustering_grid[label])\n",
    "    \n",
    "#total_existing = 0\n",
    "total_reg_penalty = 0\n",
    "#failures = 0\n",
    "    \n",
    "for index, row in clustering_grid.iterrows():\n",
    "    #if index > 20:\n",
    "    #    break\n",
    "    lat = row.Latitude  \n",
    "    lon = row.Longitude\n",
    "    my_identity = row[label]\n",
    "\n",
    "    try:\n",
    "        lat_above = np.min([i for i in lats if i > lat])\n",
    "    except:\n",
    "        #failures = failures + 1\n",
    "        lat_above = 0.0\n",
    "    try:\n",
    "        lat_below = np.max([i for i in lats if i < lat])\n",
    "    except:\n",
    "        #failures = failures + 1\n",
    "        lat_below = 0.0\n",
    "    try:\n",
    "        lon_above = np.min([i for i in lons if i > lon])\n",
    "    except:\n",
    "        #failures = failures + 1\n",
    "        lon_above = 0.0\n",
    "    try:\n",
    "        lon_below = np.max([i for i in lons if i < lon])\n",
    "    except:\n",
    "        #failures = failures + 1\n",
    "        lon_below = 0.0\n",
    "\n",
    "    lat_list = [i for i in [lat, lat_above, lat_below] if i != 0.0]\n",
    "    lon_list = [i for i in [lon, lon_above, lon_below] if i != 0.0]\n",
    "        \n",
    "    contribution = 0\n",
    "    neighboring_identities = []\n",
    "    for latitude in lat_list:\n",
    "        lat_cut = clustering_grid[clustering_grid.Latitude == latitude]\n",
    "        for longitude in lon_list:\n",
    "            #allegiance = data.loc[(data.Latitude == latitude) & (data.Longitude == longitude)][cluster_col]\n",
    "            lon_cut = lat_cut[lat_cut.Longitude == longitude]\n",
    "            allegiance = list(lon_cut[label])\n",
    "            neighboring_identities.extend(allegiance)\n",
    "            \n",
    "            if len(allegiance) == 1:\n",
    "                if not allegiance[0] == my_identity:\n",
    "                    contribution = contribution + 1\n",
    "    if contribution > 6:\n",
    "        # reassign to something in the list of neighboring identites\n",
    "        #most_common = max(set(neighboring_identities), key=neighboring_identities.count)\n",
    "        \n",
    "        # reassign to neighborhood with closest centroid\n",
    "        dist_from_cent = {}\n",
    "        for nei in list(set(neighboring_identities)):\n",
    "            points_in_clust = clustering_grid[clustering_grid[label] == nei][['Latitude','Longitude','All Crime','Battery','Theft','Narcotics']]\n",
    "            data_for_norm6 = deepcopy(points_in_clust)[['Latitude', 'Longitude', 'All Crime', 'Battery', 'Theft', 'Narcotics']]\n",
    "            for col in ['All Crime', 'Battery', 'Theft', 'Narcotics']:\n",
    "                data_for_norm6[col] = (data_for_norm6[col]-data_for_norm6[col].mean())/data_for_norm6[col].std()\n",
    "            \n",
    "            centroid = np.mean(data_for_norm6, axis=0)\n",
    "            \n",
    "            hav = hav_dist(lat, lon, centroid[0], centroid[1])\n",
    "            v1 = np.array([row['All Crime'], row['Battery'], row['Theft'], row['Narcotics']])\n",
    "            v2 = np.array(centroid[2:])\n",
    "            vec = np.linalg.norm(v1-v2)\n",
    "            dist = hav + vec\n",
    "            \n",
    "            dist_from_cent[nei] = dist\n",
    "        most_common = min(dist_from_cent, key=dist_from_cent.get)\n",
    "        if not my_identity == most_common:\n",
    "            print(\"Updating this from \" + str(my_identity) + \" to \" + str(most_common))\n",
    "        clustering_grid.at[index, new_label] = most_common\n",
    "print(\"Total time: \" + str(time.time() - s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = regularize(clustering_grid, new_label)\n",
    "\n",
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=new_label, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()\n",
    "\n",
    "# Get baseline\n",
    "print(\"Number of Neighborhoods: \" + str(len(np.unique(list(clustering_grid[new_label])))))\n",
    "all_c_emd7 = emd(clustering_grid[clustering_grid[new_label] > -1], new_label)\n",
    "all_c_sd7 = sd(clustering_grid[clustering_grid[new_label] > -1], new_label)\n",
    "print(\"The average neighborhood all crime EMD is: \" + str(np.mean(all_c_emd7)) + \", the median is: \" + str(np.median(all_c_emd7)) + \", the maximum is: \" + str(np.max(all_c_emd7)) + \", and the minimum is: \" + str(np.min(all_c_emd7)))\n",
    "print(\"The average neighborhood all crime Standard Deviation is: \" + str(np.mean(all_c_sd7)) + \", the median is: \" + str(np.median(all_c_sd7)) + \", the maximum is: \" + str(np.max(all_c_sd7)) + \", and the minimum is: \" + str(np.min(all_c_sd7)))\n",
    "print(\"The regularization parameter is \" + str(reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"Longitude\", y=\"Latitude\", data=clustering_grid, fit_reg=False, hue=new_label, legend=False, scatter_kws={\"s\": 80}, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_1s = [0.00, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "lat_2s = [0.00, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "lon_1s = [0.00, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "lon_2s = [0.00, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "euc_list = []\n",
    "hav_list = []\n",
    "\n",
    "for lat_1 in lat_1s:\n",
    "    for lat_2 in lat_2s:\n",
    "        for lon_1 in lon_1s:\n",
    "            for lon_2 in lon_2s:\n",
    "                hav_list.append(hav_dist(lat_1, lon_1, lat_2, lon_2))\n",
    "                a = np.array([lat_1, lon_1])\n",
    "                b = np.array([lat_2, lon_2])\n",
    "                euc_list.append(np.linalg.norm(a-b))\n",
    "                \n",
    "fig, axs = plt.subplots(1,2, figsize=(12,7))\n",
    "axs[0].plot(range(len(euc_list)), euc_list)\n",
    "axs[0].set_title('Euclidean Distance')\n",
    "axs[1].plot(range(len(hav_list)), hav_list)\n",
    "axs[1].set_title('Haversine Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clustering_grid[clustering_grid['Battery'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(clustering_grid['All Crime']), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
